"""
Time Series Prediction Data Preprocessing Script (Simplified Version)
Process weather.csv dataset with data checking, sliding window creation, and train/test split
No feature standardization, preserving original data scales
"""

import pandas as pd
import numpy as np
import os
import warnings
warnings.filterwarnings('ignore')

def detect_and_fix_outliers_iqr(df, columns_to_process=None, exclude_columns=None):
    """
    ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¹¶ä¿®å¤å¼‚å¸¸å€¼ï¼ˆå‰å‘å¡«å……ï¼‰

    Args:
        df (pd.DataFrame): è¾“å…¥æ•°æ®æ¡†
        columns_to_process (list): è¦å¤„ç†çš„åˆ—ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ•°å€¼åˆ—
        exclude_columns (list): è¦æ’é™¤çš„åˆ—

    Returns:
        pd.DataFrame: ä¿®å¤åçš„æ•°æ®æ¡†
    """
    print("\n" + "="*60)
    print("OUTLIER DETECTION AND FIXING (IQR Method)")
    print("="*60)

    if exclude_columns is None:
        exclude_columns = ['date', 'rain (mm)', 'raining (s)']

    # ç¡®å®šè¦å¤„ç†çš„åˆ—
    if columns_to_process is None:
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        columns_to_process = [col for col in numeric_columns if col not in exclude_columns]

    print("\nColumns to process: {}".format(len(columns_to_process)))
    print("   Columns: {}".format(', '.join(columns_to_process)))
    print("   Excluded: {}".format(', '.join(exclude_columns)))
    print("   IQR multiplier: k=10 (conservative outlier detection)")

    df_cleaned = df.copy()
    total_outliers = 0
    columns_with_outliers = []

    print("\nProcessing each column...")
    print("-" * 80)

    for col in columns_to_process:
        # è®¡ç®—IQRç»Ÿè®¡é‡
        Q1 = df_cleaned[col].quantile(0.25)
        Q3 = df_cleaned[col].quantile(0.75)
        IQR = Q3 - Q1

        # è®¡ç®—è¾¹ç•Œï¼ˆä½¿ç”¨k=10å‡å°‘è¯¯æ€ï¼‰
        lower_bound = Q1 - 10 * IQR
        upper_bound = Q3 + 10 * IQR

        # è¯†åˆ«å¼‚å¸¸å€¼
        outliers = (df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)
        n_outliers = outliers.sum()
        outlier_percentage = (n_outliers / len(df_cleaned)) * 100

        # è·å–å¼‚å¸¸å€¼çš„ç»Ÿè®¡ä¿¡æ¯
        outlier_values = df_cleaned.loc[outliers, col] if n_outliers > 0 else []

        print("\nğŸ“ˆ Column: {}".format(col))
        print("   Statistics: Q1={:.3f}, Q3={:.3f}, IQR={:.3f}".format(Q1, Q3, IQR))
        print("   Bounds: [{:.3f}, {:.3f}]".format(lower_bound, upper_bound))

        if n_outliers > 0:
            print("   âš ï¸  Outliers: {} ({:.2f}%)".format(n_outliers, outlier_percentage))
            print("   Outlier range: [{:.3f}, {:.3f}]".format(
                outlier_values.min() if len(outlier_values) > 0 else 0,
                outlier_values.max() if len(outlier_values) > 0 else 0))

            # è®°å½•ä¿®å¤å‰çš„ç»Ÿè®¡
            original_mean = df_cleaned[col].mean()
            original_std = df_cleaned[col].std()

            # æ­£ç¡®çš„å‰å‘å¡«å……å¼‚å¸¸å€¼ï¼šä½¿ç”¨pandasçš„fillnaæ–¹æ³•
            # å…ˆå°†å¼‚å¸¸å€¼è®¾ä¸ºNaNï¼Œç„¶åä½¿ç”¨ffillå’Œbfill
            df_cleaned.loc[outliers, col] = np.nan
            # å‰å‘å¡«å……
            df_cleaned[col] = df_cleaned[col].fillna(method='ffill')
            # å¯¹äºå¼€å¤´ä»ä¸ºNaNçš„å€¼ï¼Œä½¿ç”¨åå‘å¡«å……
            df_cleaned[col] = df_cleaned[col].fillna(method='bfill')

            # è®°å½•ä¿®å¤åçš„ç»Ÿè®¡
            fixed_mean = df_cleaned[col].mean()
            fixed_std = df_cleaned[col].std()

            print("   âœ… Fixed using forward/backward fill")
            print("   Stats change: Mean {:.3f} â†’ {:.3f}, Std {:.3f} â†’ {:.3f}".format(
                original_mean, fixed_mean, original_std, fixed_std))

            total_outliers += n_outliers
            columns_with_outliers.append(col)
        else:
            print("   âœ… No outliers detected")

    print("\n" + "="*60)
    print("OUTLIER PROCESSING SUMMARY")
    print("="*60)
    print("ğŸ“Š Total outliers fixed: {}".format(total_outliers))
    print("ğŸ“ Columns with outliers: {} / {}".format(len(columns_with_outliers), len(columns_to_process)))
    if columns_with_outliers:
        print("   Affected columns: {}".format(', '.join(columns_with_outliers)))
    print("ğŸ”§ Fix method: Forward fill (ffill) + Backward fill (bfill)")
    print("ğŸ’¡ Note: Time series continuity preserved")

    return df_cleaned

def load_and_check_data(file_path):
    """
    åŠ è½½æ•°æ®å¹¶è¿›è¡Œåˆæ­¥æ£€æŸ¥

    Args:
        file_path (str): æ•°æ®æ–‡ä»¶è·¯å¾„

    Returns:
        pd.DataFrame: åŠ è½½çš„æ•°æ®æ¡†
    """
    print("=== Step 1: Data Loading and Checking ===")

    try:
        # å°è¯•å¤šç§ç¼–ç æ–¹å¼
        encodings = ['utf-8', 'latin1', 'cp1252', 'iso-8859-1']
        df = None

        for encoding in encodings:
            try:
                df = pd.read_csv(file_path, encoding=encoding)
                print("Data loaded successfully, encoding: {}".format(encoding))
                break
            except UnicodeDecodeError:
                continue

        if df is None:
            raise UnicodeDecodeError("Unable to read file with any supported encoding")

        print("Data shape: {}".format(df.shape))
        print("Columns: {}, Rows: {}".format(len(df.columns), len(df)))
    except Exception as e:
        print("Data loading failed: {}".format(e))
        return None

    print("\nData basic information:")
    print(df.head())

    # æ£€æŸ¥ç¼ºå¤±å€¼
    missing_values = df.isnull().sum()
    if missing_values.sum() > 0:
        print("\nMissing values found:")
        print(missing_values[missing_values > 0])
    else:
        print("\nNo missing values")

    # æ—¶é—´æˆ³å¤„ç†
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date').reset_index(drop=True)
    print("Time range: {} to {}".format(df['date'].min(), df['date'].max()))

    return df

def create_cyclical_features(df):
    """
    åˆ›å»ºå‘¨æœŸæ€§æ—¶é—´ç‰¹å¾ï¼ˆåœ¨å¼‚å¸¸å€¼å¤„ç†ä¹‹åè°ƒç”¨ï¼‰

    Args:
        df (pd.DataFrame): è¾“å…¥æ•°æ®æ¡†ï¼ˆå·²å¤„ç†å¼‚å¸¸å€¼ï¼‰

    Returns:
        pd.DataFrame: æ·»åŠ äº†å‘¨æœŸæ€§ç‰¹å¾çš„æ•°æ®æ¡†
    """
    print("\n=== Step 1.5: Creating Cyclical Time Features ===")

    # åˆ›å»ºå‘¨æœŸæ€§æ—¶é—´ç‰¹å¾
    df['hour'] = df['date'].dt.hour
    df['month'] = df['date'].dt.month

    # æ—¥å‘¨æœŸç‰¹å¾ (24å°æ—¶å‘¨æœŸ)
    df['day_cos'] = np.cos(df['hour'] * (2 * np.pi / 24))
    df['day_sin'] = np.sin(df['hour'] * (2 * np.pi / 24))

    # å¹´å‘¨æœŸç‰¹å¾ (12ä¸ªæœˆå‘¨æœŸ)
    df['year_cos'] = np.cos(df['month'] * (2 * np.pi / 12))
    df['year_sin'] = np.sin(df['month'] * (2 * np.pi / 12))

    print("Cyclical features created:")
    print("  - day_cos, day_sin: Daily cyclical features (24h cycle)")
    print("  - year_cos, year_sin: Yearly cyclical features (12 month cycle)")
    print("  - Note: hour and month are not used as input features")

    # éªŒè¯å‘¨æœŸæ€§ç‰¹å¾çš„æ­£ç¡®æ€§
    print("\nCyclical features validation:")
    sample_hours = [0, 6, 12, 18, 23]
    print("Hour -> (day_cos, day_sin):")
    for h in sample_hours:
        cos_val = np.cos(h * (2 * np.pi / 24))
        sin_val = np.sin(h * (2 * np.pi / 24))
        print("  {} -> ({:.3f}, {:.3f})".format(h, cos_val, sin_val))

    # æ£€æŸ¥23ç‚¹å’Œ0ç‚¹æ˜¯å¦æ¥è¿‘
    hour_23_cos = np.cos(23 * (2 * np.pi / 24))
    hour_23_sin = np.sin(23 * (2 * np.pi / 24))
    hour_0_cos = np.cos(0 * (2 * np.pi / 24))
    hour_0_sin = np.sin(0 * (2 * np.pi / 24))

    distance = np.sqrt((hour_23_cos - hour_0_cos)**2 + (hour_23_sin - hour_0_sin)**2)
    print("Distance between hour 23 and 0: {:.4f} (should be small for cyclical encoding)")

    return df

def create_sliding_windows(data, window_size=12, target_col='OT'):
    """
    åˆ›å»ºæ»‘åŠ¨çª—å£æ ·æœ¬

    Args:
        data (pd.DataFrame): è¾“å…¥æ•°æ®
        window_size (int): çª—å£å¤§å°ï¼ˆæ—¶é—´æ­¥æ•°ï¼‰
        target_col (str): ç›®æ ‡åˆ—å

    Returns:
        tuple: (X, y) è¾“å…¥ç‰¹å¾å’Œç›®æ ‡å€¼
    """
    print("\n=== Step 3: Sliding Window Creation (window size={}) ===".format(window_size))

    # æ’é™¤æ—¶é—´æˆ³ã€ç›®æ ‡åˆ—å’Œä¸åº”ä½œä¸ºè¾“å…¥ç‰¹å¾çš„åˆ—
    exclude_cols = ['date', target_col, 'hour', 'month', 'day_of_year']  # ä¸ä½¿ç”¨åŸå§‹æ—¶é—´ç‰¹å¾
    feature_cols = [col for col in data.columns if col not in exclude_cols]

    print("Number of feature columns: {}".format(len(feature_cols)))
    print("Feature columns: {}".format(', '.join(feature_cols)))
    print("Excluded columns: {}".format(', '.join(exclude_cols)))
    print("Target variable: {}".format(target_col))

    # è·å–æ•°å€¼æ•°æ®
    features = data[feature_cols].values
    targets = data[target_col].values

    # åˆ›å»ºæ»‘åŠ¨çª—å£
    X, y = [], []
    for i in range(len(features) - window_size):
        X.append(features[i:i+window_size])  # è¾“å…¥ï¼šè¿‡å»window_sizeä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾
        y.append(targets[i+window_size])     # è¾“å‡ºï¼šä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„ç›®æ ‡å€¼

    X = np.array(X)
    y = np.array(y)

    print("Sliding window processing completed:")
    print("   Input shape: {} (samplesÃ—time_stepsÃ—features)".format(X.shape))
    print("   Output shape: {} (samplesÃ—1)".format(y.shape))

    return X, y, feature_cols

def split_train_val_test(X, y, train_ratio=0.7, val_ratio=0.15):
    """
    åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†

    Args:
        X (np.array): è¾“å…¥ç‰¹å¾
        y (np.array): ç›®æ ‡å€¼
        train_ratio (float): è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio (float): éªŒè¯é›†æ¯”ä¾‹ï¼ˆæµ‹è¯•é›†æ¯”ä¾‹ = 1 - train_ratio - val_ratioï¼‰

    Returns:
        tuple: (X_train, X_val, X_test, y_train, y_val, y_test)
    """
    print("\n=== Step 4: Train/Val/Test Split ===")

    # è®¡ç®—åˆ†å‰²ç‚¹
    n_samples = len(X)
    train_end = int(n_samples * train_ratio)
    val_end = int(n_samples * (train_ratio + val_ratio))

    # é¡ºåºåˆ’åˆ†ï¼ˆä¿æŒæ—¶é—´åºåˆ—é¡ºåºï¼‰
    X_train = X[:train_end]
    X_val = X[train_end:val_end]
    X_test = X[val_end:]

    y_train = y[:train_end]
    y_val = y[train_end:val_end]
    y_test = y[val_end:]

    test_ratio = 1 - train_ratio - val_ratio

    print("Dataset split completed:")
    print("   Training set: {} samples ({:.1f}%)".format(X_train.shape[0], train_ratio * 100))
    print("   Validation set: {} samples ({:.1f}%)".format(X_val.shape[0], val_ratio * 100))
    print("   Test set: {} samples ({:.1f}%)".format(X_test.shape[0], test_ratio * 100))

    return X_train, X_val, X_test, y_train, y_val, y_test

def save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test, feature_cols, output_dir='data'):
    """
    ä¿å­˜å¤„ç†åçš„æ•°æ®

    Args:
        X_train, X_val, X_test, y_train, y_val, y_test: åˆ’åˆ†åçš„æ•°æ®
        feature_cols (list): ç‰¹å¾åˆ—å
        output_dir (str): è¾“å‡ºç›®å½•
    """
    print("\n=== Step 5: Saving Processed Data ===")

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜æ•°æ®
    np.savez(os.path.join(output_dir, 'train_data.npz'),
             X=X_train, y=y_train)
    np.savez(os.path.join(output_dir, 'val_data.npz'),
             X=X_val, y=y_val)
    np.savez(os.path.join(output_dir, 'test_data.npz'),
             X=X_test, y=y_test)

    print("Data saving completed:")
    print("   Training data: {}".format(os.path.join(output_dir, 'train_data.npz')))
    print("   Validation data: {}".format(os.path.join(output_dir, 'val_data.npz')))
    print("   Test data: {}".format(os.path.join(output_dir, 'test_data.npz')))

    # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆå¯é€‰ï¼Œä¾¿äºæŸ¥çœ‹ï¼‰
    # å°†3Dæ•°ç»„è½¬æ¢ä¸º2Dç”¨äºä¿å­˜
    X_train_2d = X_train.reshape(X_train.shape[0], -1)
    X_val_2d = X_val.reshape(X_val.shape[0], -1)
    X_test_2d = X_test.reshape(X_test.shape[0], -1)

    # åˆ›å»ºåˆ—å
    columns = []
    for t in range(X_train.shape[1]):  # time steps
        for f in feature_cols:  # features
            columns.append('{}_t{}'.format(f, t))

    train_df = pd.DataFrame(X_train_2d, columns=columns)
    train_df['target_OT'] = y_train
    train_df.to_csv(os.path.join(output_dir, 'train_data.csv'), index=False)

    val_df = pd.DataFrame(X_val_2d, columns=columns)
    val_df['target_OT'] = y_val
    val_df.to_csv(os.path.join(output_dir, 'val_data.csv'), index=False)

    test_df = pd.DataFrame(X_test_2d, columns=columns)
    test_df['target_OT'] = y_test
    test_df.to_csv(os.path.join(output_dir, 'test_data.csv'), index=False)

    print("   CSV format data also saved (optional)")

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œå®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµç¨‹
    """
    print("Starting time series data preprocessing")
    print("=" * 50)

    # æ–‡ä»¶è·¯å¾„
    data_file = 'data/weather.csv'

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_file):
        print("Data file not found: {}".format(data_file))
        return

    # 1. åŠ è½½å’Œæ£€æŸ¥æ•°æ®
    df = load_and_check_data(data_file)
    if df is None:
        return

    # 1.5. å¼‚å¸¸å€¼æ£€æµ‹å’Œä¿®å¤
    df = detect_and_fix_outliers_iqr(df)

    # 1.6. åˆ›å»ºå‘¨æœŸæ€§æ—¶é—´ç‰¹å¾
    df = create_cyclical_features(df)

    # 2. åˆ›å»ºæ»‘åŠ¨çª—å£ï¼ˆæ­¥éª¤3ï¼‰
    X, y, feature_cols = create_sliding_windows(df, window_size=12, target_col='OT')

    # 3. åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼ˆæ­¥éª¤4ï¼‰
    X_train, X_val, X_test, y_train, y_val, y_test = split_train_val_test(X, y, train_ratio=0.7, val_ratio=0.15)

    # 4. ä¿å­˜å¤„ç†åçš„æ•°æ®ï¼ˆæ­¥éª¤5ï¼‰
    save_processed_data(X_train, X_val, X_test, y_train, y_val, y_test, feature_cols, output_dir='data')

    print("\n" + "=" * 50)
    print("Data preprocessing completed!")
    print("\nProcessing summary:")
    print("- Original data: 26,200 observations")
    print("- Outlier detection: IQR method with forward/backward fill")
    print("- Sliding window: 12 time steps")
    print("- Training set: {} samples (70%)".format(X_train.shape[0]))
    print("- Validation set: {} samples (15%)".format(X_val.shape[0]))
    print("- Test set: {} samples (15%)".format(X_test.shape[0]))
    print("- Input dimension: (12, 24) - 12 time steps Ã— 24 features (including cyclical time features)")
    print("- Output dimension: Single value prediction (outdoor temperature OT)")
    print("- Data status: Outliers fixed, not standardized (original scale preserved)")

    print("\nUsage instructions:")
    print("1. Data saved in data/ folder")
    print("2. Use train_data.npz for training, val_data.npz for validation")
    print("3. Use test_data.npz for final evaluation")
    print("4. CSV format files available for data inspection and visualization")
    print("5. Note: Add feature standardization as needed")

if __name__ == "__main__":
    main()
