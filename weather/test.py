#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½
ä½¿ç”¨24ä¸ªç‰¹å¾ï¼ˆ20ä¸ªæ°”è±¡ç‰¹å¾ + 4ä¸ªå‘¨æœŸæ€§æ—¶é—´ç‰¹å¾ï¼‰
"""

import numpy as np
try:
    from tensorflow.keras.models import load_model
    TENSORFLOW_AVAILABLE = True
except ImportError:
    print("Warning: TensorFlow not available, cannot load models")
    TENSORFLOW_AVAILABLE = False
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    print("Warning: matplotlib not available, plotting will be disabled")
    MATPLOTLIB_AVAILABLE = False
    plt = None
import os
import warnings
warnings.filterwarnings('ignore')

def load_model_and_scaler():
    """åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å‚æ•°"""
    print("Loading model and scaler...")

    if not TENSORFLOW_AVAILABLE:
        raise ImportError("TensorFlow is required but not available")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = 'save/model.keras'  
    scaler_path = 'save/scaler_params.npz'

    if not os.path.exists(model_path):
        raise FileNotFoundError("model file '{}' not found. Please run train.py first.".format(model_path))

    if not os.path.exists(scaler_path):
        raise FileNotFoundError("scaler parameters file '{}' not found. Please run train.py first.".format(scaler_path))

    # åŠ è½½æ¨¡å‹
    model = load_model(model_path)

    print("model loaded successfully from {}".format(model_path))

    # åŠ è½½æ ‡å‡†åŒ–å‚æ•°
    scaler_params = np.load(scaler_path)
    print("Scaler parameters loaded successfully from {}".format(scaler_path))

    return model, scaler_params

def load_test_data():
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print("Loading test data...")

    # æ£€æŸ¥æµ‹è¯•æ•°æ®æ˜¯å¦å­˜åœ¨
    test_data_path = 'data/test_data.npz'
    if not os.path.exists(test_data_path):
        raise FileNotFoundError("Test data file not found: {}".format(test_data_path))

    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data = np.load(test_data_path)
    X_test = test_data['X']
    y_test_raw = test_data['y']  # åŸå§‹æœªç»æ ‡å‡†åŒ–çš„æ•°æ®

    print("Test data loaded successfully")
    print("Test samples: {}".format(X_test.shape[0]))
    print("Sequence length: {}".format(X_test.shape[1]))
    print("Features per timestep: {}".format(X_test.shape[2]))
    print("Raw y_test range: [{:.2f}, {:.2f}]".format(y_test_raw.min(), y_test_raw.max()))

    return X_test, y_test_raw

def evaluate_model_performance(model, X_test, y_test_raw, scaler_params):
    """è¯„ä¼°æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„æ€§èƒ½"""
    print("\n=== Evaluating Model Performance ===")

    # ä½¿ç”¨ä¿å­˜çš„æ ‡å‡†åŒ–å‚æ•°å¯¹æµ‹è¯•æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–
    print("Applying standardization to test data...")
    n_samples, n_timesteps, n_features = X_test.shape

    # å¯¹X_testè¿›è¡Œæ ‡å‡†åŒ–ï¼ˆç‰¹å¾çº§åˆ«ï¼‰
    X_test_scaled = X_test.copy()
    for i in range(n_features):
        X_test_scaled[:, :, i] = (X_test[:, :, i] - scaler_params['feature_means'][i]) / (scaler_params['feature_stds'][i] + 1e-8)

    # å¯¹y_testè¿›è¡Œæ ‡å‡†åŒ–
    y_test_scaled = (y_test_raw - scaler_params['y_mean']) / (scaler_params['y_scale'] + 1e-8)

    # è¿›è¡Œé¢„æµ‹ï¼ˆåœ¨æ ‡å‡†åŒ–ç©ºé—´ï¼‰
    print("Making predictions with model...")
    y_pred_scaled = model.predict(X_test_scaled, verbose=1).flatten()

    # åæ ‡å‡†åŒ–é¢„æµ‹ç»“æœå’ŒçœŸå®å€¼åˆ°åŸå§‹å°ºåº¦
    y_pred_original = y_pred_scaled * scaler_params['y_scale'] + scaler_params['y_mean']
    y_test_original = y_test_scaled * scaler_params['y_scale'] + scaler_params['y_mean']

    print("\nData scale information:")
    print("  Raw y_test range: [{:.2f}, {:.2f}]".format(y_test_raw.min(), y_test_raw.max()))
    print("  Standardized y_test range: [{:.2f}, {:.2f}]".format(y_test_scaled.min(), y_test_scaled.max()))
    print("  Original scale y_test range: [{:.2f}, {:.2f}]".format(y_test_original.min(), y_test_original.max()))
    print("  y_mean: {:.2f}, y_scale: {:.2f}".format(scaler_params['y_mean'], scaler_params['y_scale']))

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mae = mean_absolute_error(y_test_original, y_pred_original)
    mse = mean_squared_error(y_test_original, y_pred_original)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test_original, y_pred_original)

    print("\n=== Test Results (Original Scale) ===")
    print("Mean Absolute Error (MAE): {:.4f}".format(mae))
    print("Root Mean Squared Error (RMSE): {:.4f}".format(rmse))
    print("R-squared (RÂ²): {:.4f}".format(r2))

    # æ€§èƒ½è¯„ä¼°
    print("\n=== Model Performance Assessment ===")
    if mae < 5.0:
        print("âœ“ MAE: Excellent (< 5.0Â°C)")
    elif mae < 10.0:
        print("âœ“ MAE: Good (< 10.0Â°C)")
    else:
        print("âš  MAE: Needs improvement (> 10.0Â°C)")

    if r2 > 0.7:
        print("âœ“ RÂ²: Excellent (> 0.7)")
    elif r2 > 0.5:
        print("âœ“ RÂ²: Good (> 0.5)")
    else:
        print("âš  RÂ²: Needs improvement (< 0.5)")

    # ç»Ÿè®¡é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
    errors = y_test_original - y_pred_original
    print("\n=== Error Statistics ===")
    print("Mean error: {:.4f}".format(np.mean(errors)))
    print("Median error: {:.4f}".format(np.median(errors)))
    print("Error std: {:.4f}".format(np.std(errors)))
    print("Max positive error: {:.4f}".format(np.max(errors)))
    print("Max negative error: {:.4f}".format(np.min(errors)))

    # è®¡ç®—å‡†ç¡®ç‡åŒºé—´
    accuracy_5deg = np.mean(np.abs(errors) <= 5.0) * 100
    accuracy_10deg = np.mean(np.abs(errors) <= 10.0) * 100

    print("\n=== Accuracy Analysis ===")
    print("Predictions within Â±5: {:.1f}%".format(accuracy_5deg))
    print("Predictions within Â±10: {:.1f}%".format(accuracy_10deg))

    # ç”Ÿæˆé¢„æµ‹ç»“æœå¯è§†åŒ–
    plot_predictions(y_test_original, y_pred_original, num_samples=200)

    return {
        'mae': mae,
        'rmse': rmse,
        'r2': r2,
        'y_pred': y_pred_original,
        'y_test': y_test_original,
        'errors': errors
    }

def plot_predictions(y_test_original, y_pred_original, num_samples=200):
    """
    ç»˜åˆ¶é¢„æµ‹ç»“æœå¯¹æ¯”ï¼ˆåŸå§‹å°ºåº¦ï¼‰

    Args:
        y_test_original: çœŸå®å€¼ï¼ˆåŸå§‹å°ºåº¦ï¼‰
        y_pred_original: é¢„æµ‹å€¼ï¼ˆåŸå§‹å°ºåº¦ï¼‰
        num_samples: æ˜¾ç¤ºçš„æ ·æœ¬æ•°é‡
    """
    print("\n=== Plotting Predictions ===")

    try:
        plt.figure(figsize=(15, 8))

        # é€‰æ‹©è¦æ˜¾ç¤ºçš„æ ·æœ¬èŒƒå›´
        start_idx = 0
        end_idx = min(num_samples, len(y_test_original))

        # ç»˜åˆ¶é¢„æµ‹vså®é™…
        plt.subplot(2, 1, 1)
        plt.plot(y_test_original[start_idx:end_idx], label='Actual OT', color='blue', alpha=0.7)
        plt.plot(y_pred_original[start_idx:end_idx], label='Predicted OT', color='red', alpha=0.7)
        plt.title('Temperature Prediction: Actual vs Predicted (Test Set)')
        plt.xlabel('Test Samples')
        plt.ylabel('Outdoor Temperature')
        plt.legend()
        plt.grid(True)

        # ç»˜åˆ¶è¯¯å·®
        plt.subplot(2, 1, 2)
        errors = y_test_original[start_idx:end_idx] - y_pred_original[start_idx:end_idx].flatten()
        plt.plot(errors, color='green', alpha=0.7)
        plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)
        plt.title('Prediction Errors (Test Set)')
        plt.xlabel('Test Samples')
        plt.ylabel('Error')
        plt.grid(True)

        plt.tight_layout()
        os.makedirs('plots', exist_ok=True)
        plt.savefig('plots/test_predictions.png', dpi=300, bbox_inches='tight')
        plt.show()
        print("Prediction results plot saved as 'plots/test_predictions.png'")

    except ImportError:
        print("Warning: matplotlib not available, skipping plot generation")
    except Exception as e:
        print("Error creating prediction plot: {}".format(e))

def save_test_results(results, filename='save/test_results.txt'):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
    print("\n=== Saving Test Results ===")

    # ç¡®ä¿saveæ–‡ä»¶å¤¹å­˜åœ¨
    os.makedirs('save', exist_ok=True)

    with open(filename, 'w', encoding='utf-8') as f:
        f.write("Temperature Prediction - Test Results\n")
        f.write("=" * 50 + "\n\n")
        f.write("Features: 24 (20 meteorological + 4 cyclical)\n\n")

        # è®¡ç®—é¢å¤–çš„æ€§èƒ½æŒ‡æ ‡
        from sklearn.metrics import mean_absolute_percentage_error, explained_variance_score, median_absolute_error, max_error

        mape = mean_absolute_percentage_error(results['y_test'], results['y_pred']) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        explained_var = explained_variance_score(results['y_test'], results['y_pred'])
        medae = median_absolute_error(results['y_test'], results['y_pred'])
        max_err = max_error(results['y_test'], results['y_pred'])

        f.write("Performance Metrics:\n")
        f.write("- MAE: {:.4f}\n".format(results['mae']))
        f.write("- RMSE: {:.4f}\n".format(results['rmse']))
        f.write("- RÂ²: {:.4f}\n".format(results['r2']))
        f.write("- MAPE: {:.2f}%\n".format(mape))
        f.write("- Median AE: {:.4f}\n".format(medae))
        f.write("- Explained Variance: {:.4f}\n".format(explained_var))
        f.write("- Max Error: {:.4f}\n\n".format(max_err))

        f.write("Error Statistics:\n")
        f.write("- Mean error: {:.4f}\n".format(np.mean(results['errors'])))
        f.write("- Median error: {:.4f}\n".format(np.median(results['errors'])))
        f.write("- Error std: {:.4f}\n".format(np.std(results['errors'])))
        f.write("- Max positive error: {:.4f}\n".format(np.max(results['errors'])))
        f.write("- Max negative error: {:.4f}\n\n".format(np.min(results['errors'])))

        accuracy_5deg = np.mean(np.abs(results['errors']) <= 5.0) * 100
        accuracy_10deg = np.mean(np.abs(results['errors']) <= 10.0) * 100

        f.write("Accuracy Analysis:\n")
        f.write("- Predictions within Â±5: {:.1f}%\n".format(accuracy_5deg))
        f.write("- Predictions within Â±10: {:.1f}%\n".format(accuracy_10deg))

    print("Test results saved to '{}'".format(filename))

def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œæµ‹è¯•"""
    print("ğŸ§ª Starting Model Testing")
    print("=" * 50)

    try:
        # 1. åŠ è½½æ¨¡å‹å’Œæ ‡å‡†åŒ–å‚æ•°
        model, scaler_params = load_model_and_scaler()

        # 2. åŠ è½½æµ‹è¯•æ•°æ®
        X_test, y_test = load_test_data()

        # 3. è¯„ä¼°æ¨¡å‹æ€§èƒ½
        results = evaluate_model_performance(model, X_test, y_test, scaler_params)

        # 4. ä¿å­˜æµ‹è¯•ç»“æœ
        save_test_results(results)

        print("\n" + "=" * 50)
        print(" Model Testing Completed!")
        print(" Final Results:")
        print("   MAE: {:.4f}".format(results['mae']))
        print("   RMSE: {:.4f}".format(results['rmse']))
        print("   RÂ²: {:.4f}".format(results['r2']))
        print("=" * 50)

    except Exception as e:
        print("âŒ Error during testing: {}".format(e))
        print("Please ensure you have run train.py first and all required files exist.")

if __name__ == "__main__":
    main()
